mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
mtcars90=head(mtcars,n=29) #hardcode
mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
mtcars90=head(mtcars,n=29) #hardcode
mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
mtcars90=head(mtcars,n=29) #hardcode
mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
mtcars90=head(mtcars,n=29) #hardcode
mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
mtcars90=head(mtcars,n=29) #hardcode
mtcars10=tail(mtcars,n=3)
x90=as.matrix(mtcars90[,2:ncol(mtcars90)])
x90=apply(x90,2,scale) # scaling
y90=mtcars90$mpg
x10=as.matrix(mtcars10[,2:ncol(mtcars10)])
x10=apply(x10,2,scale) # scaling
y10=mtcars10$mpg
CV=cv.glmnet(x90,y90, nfolds=5, alpha=1, family = "gaussian")
LAMBDA=CV$lambda.1se
lasso.mod=glmnet(x90,y90,alpha=1,family = "gaussian",lambda = LAMBDA)
pMPG=predict(lasso.mod,x10)
plot(y10,pMPG,xlab="actual",ylab="predicted",main="lasso")
MSE=mean((y10-pMPG)^2)
source('~/Dropbox/MataKuliah/ML/DW/lassoAssign2.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
NC
testSize
index
i<-1
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
m
n
index=rep(0,NC)
index
i=2
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
index=rep(0,NC)
m
n
index
for(i in 1:xFold){
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
index=rep(0,NC)
paste("testSize: ",testSize,"m: ",m, "n: ",n,"index: ",index)}
for(i in 1:xFold){
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
index=rep(0,NC)
print(paste("testSize: ",testSize,"m: ",m, "n: ",n,"index: ",index))}
for(i in 1:xFold){
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
index=rep(0,NC)
print(paste("testSize: ",testSize,", m: ",m, ", n: ",n,", index: ",index))}
for(i in 1:xFold){
testSize=floor(NC/xFold)
m=testSize*(i-1)+1
n=m+testSize-1
index=rep(0,NC)
print(paste("testSize: ",testSize,", m: ",m, ", n: ",n,", index: ",index))}
index
install.packages("keras")
install.packages(keras)
install.packages("keras")
R.Version()
install.packages('reticulate')
install.packages("devtools")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
install_tensorflow()
install.packages("tensorflow")
########
# Load #
########
library(keras)
data("iris")
set.seed(2021)
iris=iris[sample(1:nrow(iris)),] # shuffle data
Y <- as.numeric(iris$Species)-1
X <- as.matrix(subset(iris,select=c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width")))
Nobs <- nrow(iris) # number of observations
Nvar <- 4       # number of variables
# create training and independent test set
set <- sample(2,Nobs,replace=TRUE,prob = c(0.7,0.3))
trainingX <- X[set==1,]
trainingY <- Y[set==1]
trainingN <- length(trainingY)
testX <- X[set==2,]
testY <- Y[set==2]
testN <- length(testY)
# one-hot encoding of output variable (needs to be of type numerical=integer):
trainingY <- to_categorical(trainingY,num_classes=3) # 3 for species
testYOrig <- testY # keep original for table
testY <- to_categorical(testY,num_classes=3)
# normalize training data and apply mean and sd to test data as well
meanValues <- apply(trainingX,2,mean)
sdValues <- apply(trainingX,2,sd)
for(i in 1:Nvar) {
trainingX[,i] <- scale(trainingX[,i],center=meanValues[i],scale = sdValues[i])
testX[,i] <- scale(testX[,i],center=meanValues[i],scale = sdValues[i])
}
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
# optimizer = 'sgd',
metrics = c('accuracy')
)
history <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 300,
batch_size = 32,
validation_split=0.2)
history2 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 400,
batch_size = 32,
validation_split=0.2)
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
# optimizer = 'sgd',
metrics = c('accuracy')
)
history2 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 400,
batch_size = 32,
validation_split=0.2)
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
# optimizer = 'sgd',
metrics = c('accuracy')
)
history2 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 500,
batch_size = 32,
validation_split=0.2)
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
# optimizer = 'adam',
optimizer = 'sgd',
metrics = c('accuracy')
)
plot(history2)
plot(history2,main="500e")
plot(history2,title="500e")
history3 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 500,
batch_size = 32,
validation_split=0.2)
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
# optimizer = 'adam',
optimizer = 'sgd',
metrics = c('accuracy')
)
history4 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 900,
batch_size = 32,
validation_split=0.2)
history4
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
# optimizer = 'sgd',
metrics = c('accuracy')
)
history5 <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 900,
batch_size = 32,
validation_split=0.2)
########
# Load #
########
library(keras)
data("iris")
set.seed(2021)
iris=iris[sample(1:nrow(iris)),] # shuffle data
Y <- as.numeric(iris$Species)-1
X <- as.matrix(subset(iris,select=c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width")))
Nobs <- nrow(iris) # number of observations
Nvar <- 4       # number of variables
# create training and independent test set
set <- sample(2,Nobs,replace=TRUE,prob = c(0.7,0.3))
trainingX <- X[set==1,]
trainingY <- Y[set==1]
trainingN <- length(trainingY)
testX <- X[set==2,]
testY <- Y[set==2]
testN <- length(testY)
# one-hot encoding of output variable (needs to be of type numerical=integer):
trainingY <- to_categorical(trainingY,num_classes=3) # 3 for species
testYOrig <- testY # keep original for table
testY <- to_categorical(testY,num_classes=3)
# normalize training data and apply mean and sd to test data as well
meanValues <- apply(trainingX,2,mean)
sdValues <- apply(trainingX,2,sd)
for(i in 1:Nvar) {
trainingX[,i] <- scale(trainingX[,i],center=meanValues[i],scale = sdValues[i])
testX[,i] <- scale(testX[,i],center=meanValues[i],scale = sdValues[i])
}
model <- keras_model_sequential()
model()
model
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
# optimizer = 'sgd',
metrics = c('accuracy')
)
history <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 400,
batch_size = 32,
validation_split=0.2)
history
plot(history)
# apply to independent test dataset
model %>% evaluate(testX,testY)
prob <-model %>% predict_proba(testX)         # probabilities
predClass <- model %>% predict_classes(testX) # binary class calls
# confusion table
print(table(predicted=predClass,actual=testYOrig))
model
save_model_*
model %>% save_model_tf("test")
list.files("test")
new_model <- load_model_tf("test")
summary(new_model)
install.packages("kerastuneR")
library(kerastuneR)
install.packages("shiny")
install.packages("kerastuneR")
install.packages("magick")
install.packages("magick")
install.packages("magick")
install.packages("kerastuneR")
x_data <- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data <-  ifelse(runif(50,0,1) > 0.6, 1L,0L) %>% as.matrix()
x_data2 <- matrix(data = runif(500,0,1),nrow = 50,ncol = 5)
y_data2 <-  ifelse(runif(50,0,1) > 0.6, 1L,0L) %>% as.matrix()
library(keras)
library(kerastuneR)
library(kerastuner)
library(kerastuneR)
use_condaenv("r-reticulate")
library(kerastuneR)
library(dplyr)
use_condaenv("base")
library(kerastuneR)
devtools::install_github('henry090/kerastuneR')
library(kerastuneR)
kerastuneR::install_kerastuner(python_path = '/usr/lib/python3.8')
library(kerastuneR)
kerastuneR::install_kerastuner(python_path = '/usr/lib/python3.8')
r-reticulate
use_condaenv("r-reticulate")
library(kerastuneR)
use_condaenv("r-tensorflow")
library(kerastuneR)
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
# optimizer = 'adam',
optimizer = 'sgd',
metrics = c('accuracy')
)
history <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 1000,
batch_size = 32,
validation_split=0.2)
set.seed(2021)
iris=iris[sample(1:nrow(iris)),] # shuffle data
Y <- as.numeric(iris$Species)-1
X <- as.matrix(subset(iris,select=c("Sepal.Length","Sepal.Width","Petal.Length","Petal.Width")))
Nobs <- nrow(iris) # number of observations
Nvar <- 4       # number of variables
# create training and independent test set
set <- sample(2,Nobs,replace=TRUE,prob = c(0.7,0.3))
trainingX <- X[set==1,]
trainingY <- Y[set==1]
trainingN <- length(trainingY)
testX <- X[set==2,]
testY <- Y[set==2]
testN <- length(testY)
# one-hot encoding of output variable (needs to be of type numerical=integer):
trainingY <- to_categorical(trainingY,num_classes=3) # 3 for species
testYOrig <- testY # keep original for table
testY <- to_categorical(testY,num_classes=3)
# normalize training data and apply mean and sd to test data as well
meanValues <- apply(trainingX,2,mean)
sdValues <- apply(trainingX,2,sd)
for(i in 1:Nvar) {
trainingX[,i] <- scale(trainingX[,i],center=meanValues[i],scale = sdValues[i])
testX[,i] <- scale(testX[,i],center=meanValues[i],scale = sdValues[i])
}
model <- keras_model_sequential()
model %>%
layer_dense(units = 4, activation = 'relu', input_shape = c(Nvar)) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 3, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
# optimizer = 'adam',
optimizer = 'sgd',
metrics = c('accuracy')
)
history <- model %>% fit(
trainingX,trainingY, # now, trainingY is one-hot encoded!
epochs = 1000,
batch_size = 32,
validation_split=0.2)
history
plot(history)
model %>% evaluate(testX,testY)
prob <-model %>% predict_proba(testX)         # probabilities
predClass <- model %>% predict_classes(testX) # binary class calls
# confusion table
print(table(predicted=predClass,actual=testYOrig))
setwd("~/workspace/lightmicroscopy/LSM/Data")
F703 <- read.csv('Data/task7_0,3AU.csv')
getwd()
F703 <- read.csv('task7_0,3AU.csv')
F703
F703[1]
F703[2]
d <- density(na.omit(F703[2]), n=1e4)
d <- density(na.omit(as.numeric(F703[2])), n=1e4)
F703 <- read.csv('task7_0,3AU.csv',header=FALSE)
F703
F703 <- read.csv('task7_0,3AU.csv',header=TRUE)
F703
a <- F703[2]
a
class(a)
as.numeric(a)
as.vector(a)
F703 <- read.csv('task7_0,3AU.csv',header=TRUE,skip=1)
F703
F703 <- read.csv('task7_0,3AU.csv',header=FALSE,skip=1)
F703
d <- density(na.omit(F703[2]), n=1e4)
a <- F703[2]
a
class(a)
as.numeric(a)
b <- c(1,2,3,4)
class(b)
F703 <- read.csv('task7_0,3AU.csv',header=FALSE,skip=1)
a <- F703[2]
a<−lapply(a,as.numeric)
a <- lapply(a,as.numeric)
d <- density(na.omit(F703[2]), n=1e4)
d <- density(na.omit(a), n=1e4)
a
class(a)
as.numeric(a)
as.matrix(a) -> b
b
class(b)
b
View(b)
data.matrix(a) -> b
b
b[1]
class(b[1])
unlist(b[1])->c
c
class(c)
a <- unlist(F703[2])
a
class(a)
as.numeric(a)
a <- as.numeric(unlist(F703[2]))
a
d <- density(na.omit(a), n=1e4)
plot(d)
x <- as.numeric(unlist(F703[1]))
y <- as.numeric(unlist(F703[2]))
d <- density(na.omit(a), n=1e4)
d
View(d)
d$y
xmax <- d$x[d$y==max(d$y)]
x1 <- d$x[d$x < xmax][which.min(abs(d$y[d$x < xmax]-max(d$y)/2))]
x2 <- d$x[d$x > xmax][which.min(abs(d$y[d$x > xmax]-max(d$y)/2))]
points(c(x1, x2), c(d$y[d$x==x1], d$y[d$x==x2]), col="red")
plot(x,d$y)
length(x)
length(y)
length(d$y)
length(d$x)
# d <- density(na.omit(a), n=1e4)
plot(x,y)
xmax <- x[y==max(y)]
x2 <- x[x > xmax][which.min(abs(y[x > xmax]-max(y)/2))]
x1 <- x[x < xmax][which.min(abs(y[x < xmax]-max(y)/2))]
points(c(x1, x2), c(y[x==x1], y[x==x2]), col="red")
